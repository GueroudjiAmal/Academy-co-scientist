ions. Through these abstractions, Henson allows colo-
cated tasks on the same compute node to exchange data by simply
passing pointers to each other with zero copies.
• Libsim [9] is built on top of VisIt [10] to perform in situ visu-
alization by exposing simulation data to visualization engines.
With Libsim, visualization tasks use the same resources as the
simulation, and they take turns operating on the same time step
of data. The communication between tasks uses shared memory.
• SENSEI [11] is an in situ workflow system designed with porta-
bility in mind, targeting visualization applications. SENSEI allows
tool portability by interfacing with different in situ tools such as
ADIOS, ParaView’s Catalyst, or VisIt’s Libsim. The users can select
between those tools at runtime by specifying this information
within the configuration file. Moreover, SENSEI supports several
data communication mechanisms (e.g., shared memory, MPI).
2.1.2. Distributed workflows
Supporting scientific progress requires workflow systems to orches-
trate large and complex experiments in largely distributed infrastruc-
tures. The number of workflow management systems is large and
continuously increasing. In some cases, these systems are used mainly
by a scientific community that tailors the solution to their needs. A
sample of representative distributed workflows is listed below.
• PyCOMPSs [12] is a task-based programming model that en-
ables the development of parallel applications and workflows.
Based on annotating the Python methods and a small API, it
is able to parallelize the execution at task level and to execute
it in a distributed computing platform, such as clusters, clouds,
container-managed clusters, and more recently edge-to-cloud en-
vironments. Recent extensions to the PyCOMPSs runtime include
management of individual task faults [13], checkpointing [14],
workflow provenance [15], and task nesting [16].
• Pegasus [17] is a robust and scalable system that automates the
execution of a number of complex applications running on a
variety of heterogeneous, distributed high-throughput, and high-
performance computing environments. It is built on the principle
of separation between the workflow description and workflow
execution, providing the ability to port and adapt the workflow
based on the target execution environment.
• Parsl [18] is a parallel programming library for Python. It exposes
a task-based programming model in which developers annotate
Python functions as Parsl apps. When invoked in a standard
Python program, these apps are intercepted by Parsl and sent
to the Parsl runtime for execution. Parsl is designed for scala-
bility, with an extensible set of executors tailored to different
use cases, such as low-latency, high-throughput, or extreme-scale
execution. funcX [19] has been defined to provide a distributed
function as a service platform, using Parsl’s provider interface for
resource management. Parsl supports multiple executers, such as
Swift/TurbineExecutor [20].
• Galaxy [21] is a web-based platform initially designed for life
sciences workflows. It offers a public service and a collaborative
environment, which enables sharing of analysis tools, genomic
data, tutorial demonstrations, persistent workspaces, and publi-
cation services. Through a web browser interface, Galaxy users
can edit their workflows in a graphical editor, which are then
executed through a Galaxy server.
Future Generation Computer Systems 161 (2024) 502–513 
503 
O. Yildiz et al.
• NextFlow [22] enables scalable and reproducible scientific work-
flows using software containers. Mostly used by the life sciences
community, it allows the adaptation of pipelines written in the
most common scripting languages. Workflows are described using
a domain-specific language aiming at simplifying the implementa-
tion and the deployment of complex parallel workflows on clouds
and clusters.
• Dask [23] is an open-source Python library for parallel comput-
ing. Dask scales Python code from multicore local machines to
large distributed clusters in the cloud. Dask provides a familiar
user interface by mirroring the APIs of other libraries in the
PyData ecosystem including Pandas, scikit-learn, and NumPy.
Dask emphasizes two aspects: support for big data collections and
dynamic task scheduling.
• Autosubmit [24] is a lightweight workflow manager designed to
meet climate research needs. It integrates the capabilities of an
experiment manager, workflow orchestrator, and monitor in a
self-contained application. The experiment manager allows for
defining and configuring experiments, supported by a hierarchical
database that ensures reproducibility and traceability. The orches-
trator is designed to run workflows in research and operational
mode by managing their dependencies and interfacing with local
and remote hosts.
• RADICAL-Pilot (RP) [25] is a modular and extensible Python-
based pilot system. It is a self-contained pilot system that can
be used to provide a runtime system for workloads comprising
multiple tasks. RP can be used standalone, as well as integrated
with other application-level tools as a runtime system.
• Common Workflow Language (CWL) [26] is an open standard
for describing how to run command line tools and connect them
to create workflows. While CWL is not a workflow management
system, we believe it is relevant here because of its signifi-
cant adoption in the community. Tools and workflows described
using CWL are portable across a variety of platforms that sup-
port the CWL standards. Using CWL, a user can easily scale
complex data analysis and machine learning workflows from a
single developer’s laptop to massively parallel cluster, cloud, and
high-performance computing environments.
2.2. Data models
If all the tasks in a workflow share the same data model, the data
exchanged between the different tasks will likely use this common data
model. However, when there are different data models in the same
workflow, two possibilities exist: either selecting one of those models
and transforming all the data to it or proposing a new data model and
making all the tasks use it.
In traditional workflows, where data exchanges go through files, the
associated data model is usually inspired by the file format. For exam-
ple, workflows using HDF5 files use the HDF5 data model. Similarly, in
visualization workflows using ParaView, VTK files employ the VTK data
model. File-inspired data models can also be used for in situ workflows,
bypassing physical file storage but maintaining the file semantics. For
instance, LowFive [27] uses the HDF5 data model for in situ processing.
VTK is commonly used in ParaView Catalyst [28] and VisIt Libsim [9]
as well as in the more generic platform SENSEI [11].
Lightweight libraries such as Conduit [29] and PDI [30] have the
advantage of describing the data model independently from the ap-
plication, making them good candidates for in situ workflows. They
are generic and describe the data model in YAML or JSON format.
For instance, a simulation instrumented with PDI uses the same data
model for checkpointing into HDF5 or netCDF formats for processing
data in situ using SENSEI. ADIOS [2,31] is another example of a
system providing its own data model that can be used by the supported
backends for physical storage and in situ processing.
Bredala [32] is the data model associated with the Decaf [6] in situ
system. It annotates the data fields and sends these annotations with
the data, to protect their semantics while redistributing them.
Deisa [33], a Dask-enabled in situ processing library, is built on top
of the PDI data model and adds a virtual global data structure to the
PDI YAML to map local data into a spatiotemporal distribution. This
description is shared with the Dask analytics to protect the semantics
as well.
2.3. Data transfer mechanisms
One of the main capabilities of workflow systems is to automate
data transfers between the workflow tasks. Data transfer mechanisms
differ depending on the type of workflow system, and they also vary
among different workflow systems of the same type. In situ workflows
often communicate through shared memory or interconnect fabric,
while distributed workflows communicate through files.
Shared memory can offer benefits for workflow systems when the
tasks are on the same node by enabling zero-copy communication.
In situ solutions such as Libsim and Catalyst adopt shared-memory
communication between analysis and visualization tasks running syn-
chronously with the simulation. Henson uses shared memory to ex-
change data between the colocated tasks on the same node by dy-
namically loading the executables of these tasks into the same address
space.
When the tasks are located on separate compute nodes in the
same system, workflow systems can communicate through the system
interconnect, enabling fast communication by avoiding the parallel file
system. This communication usually takes two forms: direct messaging
and data staging. Systems such as Damaris and Decaf create a direct
communication channel among the workflow tasks and use direct
messaging via MPI to exchange data between the workflow tasks. On
the other hand, some systems such as DataSpaces [34] and Colza [35]
use a separate staging area, which is shared by the workflow tasks as
a distributed memory space.
DataSpaces, Colza, and Deisa use remote procedure calls (RPCs) for
communications. DataSpaces and Colza rely on Mercury [36] whereas
Deisa relies on Dask’s RPC implementation. These systems are con-
tributing to the recent trend to employ alternative communication
solutions rather than MPI to communicate through the interconnect.
This trend is due mainly to limitations of MPI such as lack of elasticity
or fault tolerance. The dynamicity of RPC-based systems provides an
interesting alternative for irregular applications, rather than relying on
a static and regular communication pattern of MPI.
File-based communication provides 