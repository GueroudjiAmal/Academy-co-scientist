—AI-driven scientific discovery has emerged as a
transformative fifth paradigm in research, with agentic AI
playing an increasingly prominent role across scientific domains.
Agentic AI can enable collaborative AI-human or even fully
autonomous decision-making, but it also introduces significant
reliability challenges due to the dynamic and evolutionary nature
of the AI agents. Specifically, foundation model-powered agents
are prone to generating hallucinated, misleading, or adversarial
outputs that can propagate silently through workflows and
corrupt downstream results. In this paper we present a concep-
tual framework for a unified approach that integrates agentic
workflow-level instrumentation and agent-level safeguards to
enhance the reliability of the wider system, particularly critical
in science. Embedding these mechanisms into a provenance-
augmented infrastructure enables early detection, containment,
and recovery from erroneous behavior, ultimately enhancing
reliability and reproducibility in AI-assisted scientific workflows.
Index Terms—Agentic AI, Agentic workflows, Reliability,
Safety, Agentic Systems
I. I NTRODUCTION
Building on the paradigms of empirical observation, theo-
retical modeling, computational simulation, and data-intensive
science, a fifth paradigm—AI-driven scientific discovery—has
emerged as a transformative force in research [1]–[3]. Among
AI-driven methodologies, agentic AI shows particular promise
for addressing complex scientific problems, enabling advanced
decision-making with minimal human intervention [4]–[6].
This new paradigm is playing an increasingly critical role
across a wide range of scientific disciplines, including biol-
ogy [7], chemistry [8], materials science [9], medicine [10],
and economics [11].
Agentic AI refers to intelligent systems that can au-
tonomously perceive, plan, and act to accomplish complex
goals over extended time horizons. These systems typically
involve multistep reasoning and dynamically adapt to feedback
from both their environment and external tools. An AI agent,
first introduced in 1999 [12], comprises four fundamental
This manuscript has been authored in part by UT-Battelle, LLC, under
contract DE-AC05-00OR22725 with the U.S. Department of Energy (DOE).
The publisher, by accepting the article for publication, acknowledges that the
U.S. Government retains a non-exclusive, paid up, irrevocable, worldwide
license to publish or reproduce the published form of the manuscript, or
allow others to do so, for U.S. Government purposes. The DOE will provide
public access to these results in accordance with the DOE Public Access Plan
(http://energy.gov/downloads/doe-public-access-plan).
components: (1) a brain, powered by a foundation model
(FM) such as a large language model (LLM), that enables
reasoning and decision-making; (2) memory, which stores
contextual information, past interactions, and user preferences;
(3) perception, which allows the agent to ingest and inter-
pret new inputs from external environments and tools; and
(4) action, which enables the agent to interact with other
agents and systems. These interactions often rely on structured
communication protocols, such as agent-to-agent [13] or the
Model Context Protocol (MCP) [14].
Agentic workflows integrate AI agents alongside traditional
non-AI components to orchestrate complex, multistep pro-
cesses. Unlike conventional workflow tasks and components,
AI agents exhibit dynamic and evolving behaviors driven by
continuous data integration from external sources or other
agents. These dynamic behaviors introduce significant chal-
lenges to scientific reproducibility, traceability, and, most
critically, the reliability of the overall workflow. Moreover,
these agents are prone to generating hallucinated or mis-
leading content [15]. Such outputs may silently propagate
through the system, influencing downstream computations,
decision-making, and task execution, thereby compromising
the integrity of scientific workflows. For instance, large lan-
guage models (LLMs) may output numerically plausible but
incorrect values, outputs that other agents may erroneously
treat as valid [16]–[18]. Furthermore, small variations in input
phrasing can lead to significantly different responses, making
systematic error detection more difficult [19]–[21]. Beyond
unintentional errors, these systems are also susceptible to
adversarial manipulation; malicious inputs or prompt injec-
tions can exploit vulnerabilities in LLMs, such as jailbreaking,
enabling the insertion of harmful or deceptive data that may
be further propagated by unsuspecting agents [22], [23]. In
addition to the challenges outlined above and illustrated in
Figure 1, the internal interactions within the agent, involving
the FM, memory, perception, and action components, can
introduce further irrationality and inconsistency in the agent’s
behavior. Therefore, systematically tracking and monitoring
these internal dynamics is essential to ensure explainable
decision-making and gain deeper insights into the reliability
of agentic workflows.
Beyond individual agent reliability and safety, the broader
challenge lies in designing agentic workflows that can coor-
415
2025 IEEE International Conference on eScience (eScience)
2325-3703/25/$31.00 ©2025 IEEE
DOI 10.1109/eScience65000.2025.00086
2025 IEEE International Conference on eScience (eScience) | 979-8-3315-9145-8/25/$31.00 ©2025 IEEE | DOI: 10.1109/eScience65000.2025.00086
Authorized licensed use limited to: Argonne National Laboratory. Downloaded on October 28,2025 at 21:49:58 UTC from IEEE Xplore.  Restrictions apply. 
Perception
Action
Memory
Brain
FM-powered Agent
- Hallucinations
- Inconsistency
- Math errors
- Context size
- Relevance 
- Adversarial Prompts
- Erroneous Interpretation
Dangerous Actions
Fig. 1:Schematic representation of an FM-powered agent withBrain,
Memory, Perception, and Action entities, and examples of reliability
and safety challenges.
dinate effectively, manage shared resources, resolve conflicts,
and handle redundancy while preserving traceability. These
systems must incorporate human oversight, support continual
learning, and include robust debugging and evaluation tools
to remain reliable and adaptable [24]. Addressing these inter-
connected challenges is essential to building agentic workflows
that can meaningfully accelerate scientific research while pre-
serving the rigor, transparency, verifiability, and reproducibil-
ity demanded by the scientific community.
In this paper we argue for a unified approach to enhance the
reliability and robustness of agentic workflows through inte-
grated detection, validation, control, and recovery mechanisms
in the agentic frameworks. Our objective is not to create yet
another system but to provide framework-agnostic reliability
mechanisms that can be integrated into existing frameworks
(e.g., when the underlying platform already exposes a check-
pointing mechanism, our extension simply augments that with
capabilities to check current agent accuracy, compare against
previously registered benchmark results, and then recommend
a checkpoint of the reliable state or a restart from the last most
reliable one if the accuracy dropped). Our approach introduces
both workflow-level and agent-level techniques that enable
early detection of corrupted or anomalous data, containment
of faulty behavior, and recovery of agents to known, reliable
states. Linking these mechanisms to a provenance-aware in-
frastructure enables the workflow system to trace the origin
and propagation of harmful information, whether it arises
from hallucinations, reasoning faults, or adversarial prompt
injections, and to trigger targeted recovery actions to prevent
systemwide reliability breakdown.
This work presents a conceptual framework focused on
reliability-enhancing techniques for agentic workflows. Rather
than prescribing specific implementations or tailoring solutions
to particular use cases, we articulate foundational principles
intended to guide the design of robust and trustworthy systems.
We examine how these mechanisms can be integrated into
existing agentic workflow architectures and identify key open
questions that must be addressed to enable practical deploy-
ment at scale. Through this exploration, our goal is to lay
the groundwork for future research and collaborative efforts
aimed at building scalable, reliable, and scientifically rigorous
agentic systems.
II. B ACKGROUND AND RELATED WORK
Reliability concerns in AI-powered systems arise from two
principal sources. The first stems from the inherent limitations
of current AI models and the assumption that these systems
possess true reasoning or understanding, a long-standing and
unresolved question in AI. The second involves external
threats, including adversarial attacks, data poisoning, and
prompt injection, which can compromise the trustworthiness
and safety of AI-driven workflows.
a) AI Reasoning and Rationality: AI is commonly
framed through two philosophical hypotheses; the first, known
as weak AI, suggests that machines can behave as if they
are intelligent, a widely accepted notion among researchers
today [25]. The second, called strong AI, claims that ma-
chines that appear to be intelligent are not merely simulating
intelligence but are actually capable of real thinking [25].
Reasoning-oriented models have shown remarkable perfor-
mance on complex reasoning tasks by breaking down the
problem into smaller, simpler questions and generating chains
of thoughts prior to the final answer [26], [27], which might
empower the second AI hypothesis to some extent, without
necessarily verifying it.
Despite their impressive capabilities in solving specific
tasks, current reasoning models remain far from achieving
human-level performance. In [28] the reasoning abilities of
LLMs were evaluated using cognitive tests originally de-
veloped for humans by Wason [29] and by Kahneman and
Tversky [30]. The evaluatio