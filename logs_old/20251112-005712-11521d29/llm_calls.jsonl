{"ts": "2025-11-12T00:57:42Z", "run_id": "20251112-005712-11521d29", "type": "embed_call_openai", "model": "text-embedding-3-small", "texts_sample": ["ions. Through these abstractions, Henson allows colo-\ncated tasks on the same compute node to exchange data by simply\npassing pointers to each other with zero copies.\n• Libsim [9] is built on top of VisIt [10] to perform in situ visu-\nalization by exposing simulation data to visualization engines.\nWith Libsim, visualization tasks use the same resources as the\nsimulation, and they take turns operating on the same time step\nof data. The communication between tasks uses shared memory.\n• SENSEI [11] is an in situ workflow system designed with porta-\nbility in mind, targeting visualization applications. SENSEI allows\ntool portability by interfacing with different in situ tools such as\nADIOS, ParaView’s Catalyst, or VisIt’s Libsim. The users can select\nbetween those tools at runtime by specifying this information\nwithin the configuration file. Moreover, SENSEI supports several\ndata communication mechanisms (e.g., shared memory, MPI).\n2.1.2. Distributed workflows\nSupporting scientific progress requires workflow systems to orches-\ntrate large and complex experiments in largely distributed infrastruc-\ntures. The number of workflow management systems is large and\ncontinuously increasing. In some cases, these systems are used mainly\nby a scientific community that tailors the solution to their needs. A\nsample of representative distributed workflows is listed below.\n• PyCOMPSs [12] is a task-based programming model that en-\nables the development of parallel applications and workflows.\nBased on annotating the Python methods and a small API, it\nis able to parallelize the execution at task level and to execute\nit in a distributed computing platform, such as clusters, clouds,\ncontainer-managed clusters, and more recently edge-to-cloud en-\nvironments. Recent extensions to the PyCOMPSs runtime include\nmanagement of individual task faults [13], checkpointing [14],\nworkflow provenance [15], and task nesting [16].\n• Pegasus [17] is a robust and scalable system that automates the\nexecution of a number of complex applications running on a\nvariety of heterogeneous, distributed high-throughput, and high-\nperformance computing environments. It is built on the principle\nof separation between the workflow description and workflow\nexecution, providing the ability to port and adapt the workflow\nbased on the target execution environment.\n• Parsl [18] is a parallel programming library for Python. It exposes\na task-based programming model in which developers annotate\nPython functions as Parsl apps. When invoked in a standard\nPython program, these apps are intercepted by Parsl and sent\nto the Parsl runtime for execution. Parsl is designed for scala-\nbility, with an extensible set of executors tailored to different\nuse cases, such as low-latency, high-throughput, or extreme-scale\nexecution. funcX [19] has been defined to provide a distributed\nfunction as a service platform, using Parsl’s provider interface for\nresource management. Parsl supports multiple executers, such as\nSwift/TurbineExecutor [20].\n• Galaxy [21] is a web-based platform initially designed for life\nsciences workflows. It offers a public service and a collaborative\nenvironment, which enables sharing of analysis tools, genomic\ndata, tutorial demonstrations, persistent workspaces, and publi-\ncation services. Through a web browser interface, Galaxy users\ncan edit their workflows in a graphical editor, which are then\nexecuted through a Galaxy server.\nFuture Generation Computer Systems 161 (2024) 502–513 \n503 \nO. Yildiz et al.\n• NextFlow [22] enables scalable and reproducible scientific work-\nflows using software containers. Mostly used by the life sciences\ncommunity, it allows the adaptation of pipelines written in the\nmost common scripting languages. Workflows are described using\na domain-specific language aiming at simplifying the implementa-\ntion and the deployment of complex parallel workflows on clouds\nand clusters.\n• Dask [23] is an open-source Python library for parallel comput-\ning. Dask scales Python code from multicore local machines to\nlarge distributed clusters in the cloud. Dask provides a familiar\nuser interface by mirroring the APIs of other libraries in the\nPyData ecosystem including Pandas, scikit-learn, and NumPy.\nDask emphasizes two aspects: support for big data collections and\ndynamic task scheduling.\n• Autosubmit [24] is a lightweight workflow manager designed to\nmeet climate research needs. It integrates the capabilities of an\nexperiment manager, workflow orchestrator, and monitor in a\nself-contained application. The experiment manager allows for\ndefining and configuring experiments, supported by a hierarchical\ndatabase that ensures reproducibility and traceability. The orches-\ntrator is designed to run workflows in research and operational\nmode by managing their dependencies and interfacing with local\nand remote hosts.\n• RADICAL-Pilot (RP) [25] is a modular and extensible Python-\nbased pilot system. It is a self-contained pilot system that can\nbe used to provide a runtime system for workloads comprising\nmultiple tasks. RP can be used standalone, as well as integrated\nwith other application-level tools as a runtime system.\n• Common Workflow Language (CWL) [26] is an open standard\nfor describing how to run command line tools and connect them\nto create workflows. While CWL is not a workflow management\nsystem, we believe it is relevant here because of its signifi-\ncant adoption in the community. Tools and workflows described\nusing CWL are portable across a variety of platforms that sup-\nport the CWL standards. Using CWL, a user can easily scale\ncomplex data analysis and machine learning workflows from a\nsingle developer’s laptop to massively parallel cluster, cloud, and\nhigh-performance computing environments.\n2.2. Data models\nIf all the tasks in a workflow share the same data model, the data\nexchanged between the different tasks will likely use this common data\nmodel. However, when there are different data models in the same\nworkflow, two possibilities exist: either selecting one of those models\nand transforming all the data to it or proposing a new data model and\nmaking all the tasks use it.\nIn traditional workflows, where data exchanges go through files, the\nassociated data model is usually inspired by the file format. For exam-\nple, workflows using HDF5 files use the HDF5 data model. Similarly, in\nvisualization workflows using ParaView, VTK files employ the VTK data\nmodel. File-inspired data models can also be used for in situ workflows,\nbypassing physical file storage but maintaining the file semantics. For\ninstance, LowFive [27] uses the HDF5 data model for in situ processing.\nVTK is commonly used in ParaView Catalyst [28] and VisIt Libsim [9]\nas well as in the more generic platform SENSEI [11].\nLightweight libraries such as Conduit [29] and PDI [30] have the\nadvantage of describing the data model independently from the ap-\nplication, making them good candidates for in situ workflows. They\nare generic and describe the data model in YAML or JSON format.\nFor instance, a simulation instrumented with PDI uses the same data\nmodel for checkpointing into HDF5 or netCDF formats for processing\ndata in situ using SENSEI. ADIOS [2,31] is another example of a\nsystem providing its own data model that can be used by the supported\nbackends for physical storage and in situ processing.\nBredala [32] is the data model associated with the Decaf [6] in situ\nsystem. It annotates the data fields and sends these annotations with\nthe data, to protect their semantics while redistributing them.\nDeisa [33], a Dask-enabled in situ processing library, is built on top\nof the PDI data model and adds a virtual global data structure to the\nPDI YAML to map local data into a spatiotemporal distribution. This\ndescription is shared with the Dask analytics to protect the semantics\nas well.\n2.3. Data transfer mechanisms\nOne of the main capabilities of workflow systems is to automate\ndata transfers between the workflow tasks. Data transfer mechanisms\ndiffer depending on the type of workflow system, and they also vary\namong different workflow systems of the same type. In situ workflows\noften communicate through shared memory or interconnect fabric,\nwhile distributed workflows communicate through files.\nShared memory can offer benefits for workflow systems when the\ntasks are on the same node by enabling zero-copy communication.\nIn situ solutions such as Libsim and Catalyst adopt shared-memory\ncommunication between analysis and visualization tasks running syn-\nchronously with the simulation. Henson uses shared memory to ex-\nchange data between the colocated tasks on the same node by dy-\nnamically loading the executables of these tasks into the same address\nspace.\nWhen the tasks are located on separate compute nodes in the\nsame system, workflow systems can communicate through the system\ninterconnect, enabling fast communication by avoiding the parallel file\nsystem. This communication usually takes two forms: direct messaging\nand data staging. Systems such as Damaris and Decaf create a direct\ncommunication channel among the workflow tasks and use direct\nmessaging via MPI to exchange data between the workflow tasks. On\nthe other hand, some systems such as DataSpaces [34] and Colza [35]\nuse a separate staging area, which is shared by the workflow tasks as\na distributed memory space.\nDataSpaces, Colza, and Deisa use remote procedure calls (RPCs) for\ncommunications. DataSpaces and Colza rely on Mercury [36] whereas\nDeisa relies on Dask’s RPC implementation. These systems are con-\ntributing to the recent trend to employ alternative communication\nsolutions rather than MPI to communicate through the interconnect.\nThis trend is due mainly to limitations of MPI such as lack of elasticity\nor fault tolerance. The dynamicity of RPC-based systems provides an\ninteresting alternative for irregular applications, rather than relying on\na static and regular communication pattern of MPI.\nFile-based communication provides", "Vector databases have rapidly grown in popularity, enabling effi-\ncient similarity search over data such as text, images, and video.\nThey now play a central role in modern AI workflows, aiding large\nlanguage models by grounding model outputs in external literature\nthrough retrieval-augmented generation. Despite their importance,\nlittle is known about the performance characteristics of vector\ndatabases in high-performance computing (HPC) systems that drive\nlarge-scale science. This work presents an empirical study of dis-\ntributed vector database performance on the Polaris supercomputer\nin the Argonne Leadership Computing Facility. We construct a\nrealistic biological-text workload from BV-BRC and generate em-\nbeddings from the peS2o corpus using Qwen3-Embedding-4B. We\nselect Qdrant to evaluate insertion, index construction, and query\nlatency with up to 32 workers. Informed by practical lessons from\nour experience, this work takes a first step toward characterizing\nvector database performance on HPC platforms to guide future\nresearch and optimization. 1", "Agentic artificial intelligence systems can reason, plan, and execute\ncomplex workflows, which can speed up scientific discovery. Their\nnon-deterministic behavior, however, makes reliability a core chal-\nlenge in domains that require safety, precision, and reproducibility.\nMoreover, their autonomy can cause security threats if their per-\nmissions are not correctly set up. This paper proposes a practical\nset of best-practice guardrails that aim to guarantee reliable Agen-\ntic workflows, both from the science and cybersecurity point of\nview. We organize the principles into safety, reliability, and security\nsets, from intent to actuation: intent normalization, schema en-\nforcement, and role-scoped tool access; uncertainty budgeting with\npropagation and computed thresholds; provenance capture with\nreproducible configurations, caching, and rollback; and verification\nloops that include simulation cross-checks, model recalibration,\nand human-in-the-loop escalation. We ground the guidance in a\ncase study, an Agentic chemistry framework for molecular property\nprediction and automation that integrates knowledge-driven plan-\nning with high-fidelity simulation in a closed loop. We show how\nthe guardrails reduce common failure modes, improve end-to-end\nreliability, and yield scientifically valid, reproducible outcomes. The\nresult is a concise set of patterns that practitioners can adopt to\nbuild trustworthy autonomy in scientific computing.\nCCS Concepts\n•Computing methodologies →Distributed computing method-\nologies;Parallel computing methodologies; Artificial intelligence.\nKeywords\nReliable Agentic AI, Agentic Workflows, Computational Chemistry\nACM Reference Format:\n. 2026. Agentic Workflow Design Principles for Reliable Science. InProceed-\nings of The 41st ACM/SIGAPP Symposium on Applied Computing (SAC’26).\nACM, New York, NY, USA, 8 pages. https://doi.org/XXXXXXX.XXXXXXX"], "num_texts": 8, "context": {}}
{"ts": "2025-11-12T00:57:43Z", "run_id": "20251112-005712-11521d29", "type": "embed_result_openai", "model": "text-embedding-3-small", "num_vectors": 8, "dim": 1536, "context": {}}
{"ts": "2025-11-12T00:57:43Z", "run_id": "20251112-005712-11521d29", "type": "embed_call_openai", "model": "text-embedding-3-small", "texts_sample": ["HPC workflow provenance"], "num_texts": 1, "context": {}}
{"ts": "2025-11-12T00:57:45Z", "run_id": "20251112-005712-11521d29", "type": "embed_result_openai", "model": "text-embedding-3-small", "num_vectors": 1, "dim": 1536, "context": {}}
{"ts": "2025-11-12T00:57:58Z", "run_id": "20251112-005712-11521d29", "type": "embed_call_openai", "model": "text-embedding-3-small", "texts_sample": ["HPC workflow provenance"], "num_texts": 1, "context": {}}
{"ts": "2025-11-12T00:57:59Z", "run_id": "20251112-005712-11521d29", "type": "embed_result_openai", "model": "text-embedding-3-small", "num_vectors": 1, "dim": 1536, "context": {}}
